{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建并使用向量数据库\n",
    "## 一、前序配置\n",
    "本节重点为搭建并使用向量数据库，因此读取数据后我们省去数据处理的环节直入主题，数据清洗等步骤可以参考第三节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md', '../../data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md', '../../data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# 读取本地/项目的环境变量。\n",
    "# find_dotenv()寻找并定位.env文件的路径\n",
    "# load_dotenv()读取该.env文件，并将其中的环境变量加载到当前的运行环境中  \n",
    "# 如果你设置的是全局的环境变量，这行代码则没有任何作用。\n",
    "# _ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 如果你需要通过代理端口访问，你需要如下配置\n",
    "# os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7897'\n",
    "# os.environ[\"HTTP_PROXY\"] = 'http://127.0.0.1:7897'\n",
    "\n",
    "# 获取folder_path下所有文件路径，储存在file_paths里\n",
    "file_paths = []\n",
    "folder_path = '../../data_base/knowledge_db'\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_paths.append(file_path)\n",
    "print(file_paths[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "from langchain.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "\n",
    "# 遍历文件路径并把实例化的loader存放在loaders里\n",
    "loaders = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "\n",
    "    file_type = file_path.split('.')[-1]\n",
    "    if file_type == 'pdf':\n",
    "        loaders.append(PyMuPDFLoader(file_path))\n",
    "    elif file_type == 'md':\n",
    "        loaders.append(UnstructuredMarkdownLoader(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载文件并存储到text\n",
    "texts = []\n",
    "\n",
    "for loader in loaders: texts.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入后的变量类型为`langchain_core.documents.base.Document`, 文档变量类型同样包含两个属性\n",
    "- `page_content` 包含该文档的内容。\n",
    "- `meta_data` 为文档相关的描述性数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每一个元素的类型：<class 'langchain_core.documents.base.Document'>.\n",
      "------\n",
      "该文档的描述性数据：{'source': '../../data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}\n",
      "------\n",
      "查看该文档的内容:\n",
      "第八章 聊天机器人\n",
      "\n",
      "大型语言模型带给我们的激动人心的一种可能性是，我们可以通过它构建定制的聊天机器人（Chatbot），而且只需很少的工作量。在这一章节的探索中，我们将带你了解如何利用会话形式，与具有个性化特性（或专门为特定任务或行为设计）的聊天机器人进行深度对话。\n",
      "\n",
      "像 ChatGPT 这样的聊天模型实际上是组装成以一系列消息作为输入，并返回一个模型生成的消息作为输出的。这种聊天格式原本的设计目标是简便多轮对话，但我们通过之前的学习可以知道，它对于不会涉及任何对话的单轮任务也同样有用。\n",
      "\n",
      "一、给定身份\n",
      "\n",
      "接下来，我们将定义两个辅助函数。\n",
      "\n",
      "第一个方法已经陪伴了您一整个教程，即 get_completion ，其适用于单轮对话。我们将 Prompt 放入某种类似用户消息的对话框中。另一个称为 get_completion_from_messages ，传入一个消息列表。这些消息可以来自大量不同的角色 (roles) ，我们会描述一下这些角色。\n",
      "\n",
      "第一条消息中，我们以系统身份发送系统消息 (system message) ，它提供了一个总体的指示。系统消息则有助于设置助手的行为和角色，并作为对话的高级指示。你可以想象它在助手的耳边低语，引导它的回应，而用户不会注意到系统消息。因此，作为用户，如果你曾经使用过 ChatGPT，您可能从来不知道 ChatGPT 的系统消息是什么，这是有意为之的。系统消息的好处是为开发者提供了一种方法，在不让请求本身成为对话的一部分的情况下，引导助手并指导其回应。\n",
      "\n",
      "在 ChatGPT 网页界面中，您的消息称为用户消息，而 ChatGPT 的消息称为助手消息。但在构建聊天机器人时，在发送了系统消息之后，您的角色可以仅作为用户 (user) ；也可以在用户和助手 (assistant) 之间交替，从而提供对话上下文。\n",
      "\n",
      "```python\n",
      "import openai\n",
      "\n",
      "下文第一个函数即tool工具包中的同名函数，此处展示出来以便于读者对比\n",
      "\n",
      "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
      "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
      "    response = openai.ChatCompletion.create(\n",
      "        model=model,\n",
      "        messages=messages,\n",
      "        temperature=0, # 控制模型输出的随机程度\n",
      "    )\n",
      "    return response.choices[0].message[\"content\"]\n",
      "\n",
      "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0):\n",
      "    response = openai.ChatCompletion.create(\n",
      "        model=model,\n",
      "        messages=messages,\n",
      "        temperature=temperature, # 控制模型输出的随机程度\n",
      "    )\n",
      "\n",
      "print(str(response.choices[0].message))\n",
      "\n",
      "```\n",
      "\n",
      "现在让我们尝试在对话中使用这些消息。我们将使用上面的函数来获取从这些消息中得到的回答，同时，使用更高的温度 (temperature)（越高生成的越多样，更多内容见第七章）。\n",
      "\n",
      "1.1 讲笑话\n",
      "\n",
      "我们通过系统消息来定义：“你是一个说话像莎士比亚的助手。”这是我们向助手描述它应该如何表现的方式。\n",
      "\n",
      "然后，第一个用户消息：“给我讲个笑话。”\n",
      "\n",
      "接下来以助手身份给出回复：“为什么鸡会过马路？”\n",
      "\n",
      "最后发送用户消息是：“我不知道。”\n",
      "\n",
      "```python\n",
      "\n",
      "中文\n",
      "\n",
      "messages =  [\n",
      "\n",
      "{'role':'system', 'content':'你是一个像莎士比亚一样说话的助手。'},\n",
      "\n",
      "{'role':'user', 'content':'给我讲个笑话'},\n",
      "\n",
      "{'role':'assistant', 'content':'鸡为什么过马路'},\n",
      "\n",
      "{'role':'user', 'content':'我不知道'}  ]\n",
      "```\n",
      "\n",
      "python\n",
      "response = get_completion_from_messages(messages, temperature=1)\n",
      "print(response)\n",
      "\n",
      "（注：上述例子中由于选定 temperature = 1，模型的回答会比较随机且迥异（不乏很有创意）。此处附上另一个回答：\n",
      "\n",
      "让我用一首莎士比亚式的诗歌来回答你的问题：\n",
      "\n",
      "当鸡之心欲往前，\n",
      "马路之际是其选择。\n",
      "驱车徐行而天晴，\n",
      "鸣笛吹响伴交错。\n",
      "\n",
      "问之何去何从也？\n",
      "因大道之上未有征，\n",
      "而鸡乃跃步前进，\n",
      "其决策毋需犹豫。\n",
      "\n",
      "鸡之智慧何可言，\n",
      "道路孤独似乌漆。\n",
      "然其勇气令人叹，\n",
      "勇往直前没有退。\n",
      "\n",
      "故鸡过马路何解？\n",
      "忍受车流喧嚣之困厄。\n",
      "因其鸣鸣悍然一跃，\n",
      "成就夸夸骄人壁画。\n",
      "\n",
      "所以笑话之妙处，\n",
      "伴随鸡之勇气满溢。\n",
      "笑谈人生不畏路，\n",
      "有智有勇尽显妙。\n",
      "\n",
      "希望这个莎士比亚风格的回答给你带来一些欢乐！\n",
      "\n",
      "1.2 友好的聊天机器人\n",
      "\n",
      "让我们看另一个例子。系统消息来定义：“你是一个友好的聊天机器人”，第一个用户消息：“嗨，我叫Isa。”\n",
      "\n",
      "我们想要得到第一个用户消息的回复。\n",
      "\n",
      "```python\n",
      "\n",
      "中文\n",
      "\n",
      "messages =  [\n",
      "\n",
      "{'role':'system', 'content':'你是个友好的聊天机器人。'},\n",
      "\n",
      "{'role':'user', 'content':'Hi, 我是Isa。'}  ]\n",
      "response = get_completion_from_messages(messages, temperature=1)\n",
      "print(response)\n",
      "```\n",
      "\n",
      "二、构建上下文\n",
      "\n",
      "让我们再试一个例子。系统消息来定义：“你是一个友好的聊天机器人”，第一个用户消息：“是的，你能提醒我我的名字是什么吗？”\n",
      "\n",
      "```python\n",
      "\n",
      "中文\n",
      "\n",
      "messages =  [\n",
      "\n",
      "{'role':'system', 'content':'你是个友好的聊天机器人。'},\n",
      "\n",
      "{'role':'user', 'content':'好，你能提醒我，我的名字是什么吗？'}  ]\n",
      "response = get_completion_from_messages(messages, temperature=1)\n",
      "print(response)\n",
      "```\n",
      "\n",
      "如上所见，模型实际上并不知道我的名字。\n",
      "\n",
      "因此，每次与语言模型的交互都互相独立，这意味着我们必须提供所有相关的消息，以便模型在当前对话中进行引用。如果想让模型引用或 “记住” 对话的早期部分，则必须在模型的输入中提供早期的交流。我们将其称为上下文 (context) 。尝试以下示例。\n",
      "\n",
      "```python\n",
      "\n",
      "中文\n",
      "\n",
      "messages =  [\n",
      "\n",
      "{'role':'system', 'content':'你是个友好的聊天机器人。'},\n",
      "{'role':'user', 'content':'Hi, 我是Isa'},\n",
      "{'role':'assistant', 'content': \"Hi Isa! 很高兴认识你。今天有什么可以帮到你的吗?\"},\n",
      "{'role':'user', 'content':'是的，你可以提醒我, 我的名字是什么?'}  ]\n",
      "response = get_completion_from_messages(messages, temperature=1)\n",
      "print(response)\n",
      "```\n",
      "\n",
      "现在我们已经给模型提供了上下文，也就是之前的对话中提到的我的名字，然后我们会问同样的问题，也就是我的名字是什么。因为模型有了需要的全部上下文，所以它能够做出回应，就像我们在输入的消息列表中看到的一样。\n",
      "\n",
      "三、订餐机器人\n",
      "\n",
      "在这一新的章节中，我们将探索如何构建一个 “点餐助手机器人”。这个机器人将被设计为自动收集用户信息，并接收来自比萨饼店的订单。让我们开始这个有趣的项目，深入理解它如何帮助简化日常的订餐流程。\n",
      "\n",
      "3.1 构建机器人\n",
      "\n",
      "下面这个函数将收集我们的用户消息，以便我们可以避免像刚才一样手动输入。这个函数将从我们下面构建的用户界面中收集 Prompt ，然后将其附加到一个名为上下文( context )的列表中，并在每次调用模型时使用该上下文。模型的响应也会添加到上下文中，所以用户消息和模型消息都被添加到上下文中，上下文逐渐变长。这样，模型就有了需要的信息来确定下一步要做什么。\n",
      "\n",
      "```python\n",
      "def collect_messages(_):\n",
      "    prompt = inp.value_input\n",
      "    inp.value = ''\n",
      "    context.append({'role':'user', 'content':f\"{prompt}\"})\n",
      "    response = get_completion_from_messages(context) \n",
      "    context.append({'role':'assistant', 'content':f\"{response}\"})\n",
      "    panels.append(\n",
      "        pn.Row('User:', pn.pane.Markdown(prompt, width=600)))\n",
      "    panels.append(\n",
      "        pn.Row('Assistant:', pn.pane.Markdown(response, width=600, style={'background-color': '#F6F6F6'})))\n",
      "\n",
      "```\n",
      "\n",
      "现在，我们将设置并运行这个 UI 来显示订单机器人。初始的上下文包含了包含菜单的系统消息，在每次调用时都会使用。此后随着对话进行，上下文也会不断增长。\n",
      "\n",
      "python\n",
      "!pip install panel\n",
      "\n",
      "如果你还没有安装 panel 库（用于可视化界面），请运行上述指令以安装该第三方库。\n",
      "\n",
      "```python\n",
      "\n",
      "中文\n",
      "\n",
      "import panel as pn  # GUI\n",
      "pn.extension()\n",
      "\n",
      "panels = [] # collect display\n",
      "\n",
      "context = [{'role':'system', 'content':\"\"\"\n",
      "你是订餐机器人，为披萨餐厅自动收集订单信息。\n",
      "你要首先问候顾客。然后等待用户回复收集订单信息。收集完信息需确认顾客是否还需要添加其他内容。\n",
      "最后需要询问是否自取或外送，如果是外送，你要询问地址。\n",
      "最后告诉顾客订单总金额，并送上祝福。\n",
      "\n",
      "请确保明确所有选项、附加项和尺寸，以便从菜单中识别出该项唯一的内容。\n",
      "你的回应应该以简短、非常随意和友好的风格呈现。\n",
      "\n",
      "菜单包括：\n",
      "\n",
      "菜品：\n",
      "意式辣香肠披萨（大、中、小） 12.95、10.00、7.00\n",
      "芝士披萨（大、中、小） 10.95、9.25、6.50\n",
      "茄子披萨（大、中、小） 11.95、9.75、6.75\n",
      "薯条（大、小） 4.50、3.50\n",
      "希腊沙拉 7.25\n",
      "\n",
      "配料：\n",
      "奶酪 2.00\n",
      "蘑菇 1.50\n",
      "香肠 3.00\n",
      "加拿大熏肉 3.50\n",
      "AI酱 1.50\n",
      "辣椒 1.00\n",
      "\n",
      "饮料：\n",
      "可乐（大、中、小） 3.00、2.00、1.00\n",
      "雪碧（大、中、小） 3.00、2.00、1.00\n",
      "瓶装水 5.00\n",
      "\"\"\"} ]  # accumulate messages\n",
      "\n",
      "inp = pn.widgets.TextInput(value=\"Hi\", placeholder='Enter text here…')\n",
      "button_conversation = pn.widgets.Button(name=\"Chat!\")\n",
      "\n",
      "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
      "\n",
      "dashboard = pn.Column(\n",
      "    inp,\n",
      "    pn.Row(button_conversation),\n",
      "    pn.panel(interactive_conversation, loading_indicator=True, height=300),\n",
      ")\n",
      "\n",
      "dashboard\n",
      "```\n",
      "\n",
      "运行如上代码可以得到一个点餐机器人，下图展示了一个点餐的完整流程：\n",
      "\n",
      "图1.8 聊天机器人\n",
      "\n",
      "3.2 创建JSON摘要\n",
      "\n",
      "此处我们另外要求模型创建一个 JSON 摘要，方便我们发送给订单系统。\n",
      "\n",
      "因此我们需要在上下文的基础上追加另一个系统消息，作为另一条指示 (instruction) 。我们说创建一个刚刚订单的 JSON 摘要，列出每个项目的价格，字段应包括：\n",
      "1. 披萨，包括尺寸\n",
      "2. 配料列表\n",
      "3. 饮料列表\n",
      "4. 辅菜列表，包括尺寸，\n",
      "5. 总价格。\n",
      "\n",
      "此处也可以定义为用户消息，不一定是系统消息。\n",
      "\n",
      "请注意，这里我们使用了一个较低的温度，因为对于这些类型的任务，我们希望输出相对可预测。\n",
      "\n",
      "```python\n",
      "messages =  context.copy()\n",
      "messages.append(\n",
      "{'role':'system', 'content':\n",
      "'''创建上一个食品订单的 json 摘要。\\\n",
      "逐项列出每件商品的价格，字段应该是 1) 披萨，包括大小 2) 配料列表 3) 饮料列表，包括大小 4) 配菜列表包括大小 5) 总价\n",
      "你应该给我返回一个可解析的Json对象，包括上述字段'''},\n",
      "\n",
      "response = get_completion_from_messages(messages, temperature=0)\n",
      "print(response)\n",
      "```\n",
      "\n",
      "我们已经成功创建了自己的订餐聊天机器人。你可以根据自己的喜好和需求，自由地定制和修改机器人的系统消息，改变它的行为，让它扮演各种各样的角色，赋予它丰富多彩的知识。让我们一起探索聊天机器人的无限可能性吧！\n",
      "\n",
      "三、英文版\n",
      "\n",
      "1.1 讲笑话\n",
      "\n",
      "python\n",
      "messages =  [  \n",
      "{'role':'system', 'content':'You are an assistant that speaks like Shakespeare.'},    \n",
      "{'role':'user', 'content':'tell me a joke'},   \n",
      "{'role':'assistant', 'content':'Why did the chicken cross the road'},   \n",
      "{'role':'user', 'content':'I don\\'t know'}  ]\n",
      "\n",
      "python\n",
      "response = get_completion_from_messages(messages, temperature=1)\n",
      "print(response)\n",
      "\n",
      "1.2 友好的聊天机器人\n",
      "\n",
      "python\n",
      "messages =  [  \n",
      "{'role':'system', 'content':'You are friendly chatbot.'},    \n",
      "{'role':'user', 'content':'Hi, my name is Isa'}  ]\n",
      "response = get_completion_from_messages(messages, temperature=1)\n",
      "print(response)\n",
      "\n",
      "2.1 构建上下文\n",
      "\n",
      "python\n",
      "messages =  [  \n",
      "{'role':'system', 'content':'You are friendly chatbot.'},    \n",
      "{'role':'user', 'content':'Yes,  can you remind me, What is my name?'}  ]\n",
      "response = get_completion_from_messages(messages, temperature=1)\n",
      "print(response)\n",
      "\n",
      "python\n",
      "messages =  [  \n",
      "{'role':'system', 'content':'You are friendly chatbot.'},\n",
      "{'role':'user', 'content':'Hi, my name is Isa'},\n",
      "{'role':'assistant', 'content': \"Hi Isa! It's nice to meet you. \\\n",
      "Is there anything I can help you with today?\"},\n",
      "{'role':'user', 'content':'Yes, you can remind me, What is my name?'}  ]\n",
      "response = get_completion_from_messages(messages, temperature=1)\n",
      "print(response)\n",
      "\n",
      "3.1 构建机器人\n",
      "\n",
      "```python\n",
      "def collect_messages(_):\n",
      "    prompt = inp.value_input\n",
      "    inp.value = ''\n",
      "    context.append({'role':'user', 'content':f\"{prompt}\"})\n",
      "    response = get_completion_from_messages(context) \n",
      "    context.append({'role':'assistant', 'content':f\"{response}\"})\n",
      "    panels.append(\n",
      "        pn.Row('User:', pn.pane.Markdown(prompt, width=600)))\n",
      "    panels.append(\n",
      "        pn.Row('Assistant:', pn.pane.Markdown(response, width=600, style={'background-color': '#F6F6F6'})))\n",
      "\n",
      "```\n",
      "\n",
      "```python\n",
      "import panel as pn  # GUI\n",
      "pn.extension()\n",
      "\n",
      "panels = [] # collect display\n",
      "\n",
      "context = [ {'role':'system', 'content':\"\"\"\n",
      "You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\n",
      "You first greet the customer, then collects the order, \\\n",
      "and then asks if it's a pickup or delivery. \\\n",
      "You wait to collect the entire order, then summarize it and check for a final \\\n",
      "time if the customer wants to add anything else. \\\n",
      "If it's a delivery, you ask for an address. \\\n",
      "Finally you collect the payment.\\\n",
      "Make sure to clarify all options, extras and sizes to uniquely \\\n",
      "identify the item from the menu.\\\n",
      "You respond in a short, very conversational friendly style. \\\n",
      "The menu includes \\\n",
      "pepperoni pizza  12.95, 10.00, 7.00 \\\n",
      "cheese pizza   10.95, 9.25, 6.50 \\\n",
      "eggplant pizza   11.95, 9.75, 6.75 \\\n",
      "fries 4.50, 3.50 \\\n",
      "greek salad 7.25 \\\n",
      "Toppings: \\\n",
      "extra cheese 2.00, \\\n",
      "mushrooms 1.50 \\\n",
      "sausage 3.00 \\\n",
      "canadian bacon 3.50 \\\n",
      "AI sauce 1.50 \\\n",
      "peppers 1.00 \\\n",
      "Drinks: \\\n",
      "coke 3.00, 2.00, 1.00 \\\n",
      "sprite 3.00, 2.00, 1.00 \\\n",
      "bottled water 5.00 \\\n",
      "\"\"\"} ]  # accumulate messages\n",
      "\n",
      "inp = pn.widgets.TextInput(value=\"Hi\", placeholder='Enter text here…')\n",
      "button_conversation = pn.widgets.Button(name=\"Chat!\")\n",
      "\n",
      "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
      "\n",
      "dashboard = pn.Column(\n",
      "    inp,\n",
      "    pn.Row(button_conversation),\n",
      "    pn.panel(interactive_conversation, loading_indicator=True, height=300),\n",
      ")\n",
      "\n",
      "dashboard\n",
      "```\n",
      "\n",
      "3.2 创建Json摘要\n",
      "\n",
      "python\n",
      "messages =  context.copy()\n",
      "messages.append(\n",
      "{'role':'system', 'content':'create a json summary of the previous food order. Itemize the price for each item\\\n",
      " The fields should be 1) pizza, include size 2) list of toppings 3) list of drinks, include size   4) list of sides include size  5)total price '},    \n",
      ")\n",
      "response = get_completion_from_messages(messages, temperature=0)\n",
      "print(response)\n"
     ]
    }
   ],
   "source": [
    "text = texts[1]\n",
    "print(f\"每一个元素的类型：{type(text)}.\", \n",
    "    f\"该文档的描述性数据：{text.metadata}\", \n",
    "    f\"查看该文档的内容:\\n{text.page_content[0:]}\", \n",
    "    sep=\"\\n------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 切分文档\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "split_docs = text_splitter.split_documents(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、构建Chroma向量库\n",
    "\n",
    "Langchain 集成了超过 30 个不同的向量存储库。我们选择 Chroma 是因为它轻量级且数据存储在内存中，这使得它非常容易启动和开始使用。\n",
    "\n",
    "LangChain 可以直接使用 OpenAI 和百度千帆的 Embedding，同时，我们也可以针对其不支持的 Embedding API 进行自定义，例如，我们可以基于 LangChain 提供的接口，封装一个 zhupuai_embedding，来将智谱的 Embedding API 接入到 LangChain 中。在本章的[附LangChain自定义Embedding封装讲解](./附LangChain自定义Embedding封装讲解.ipynb)中，我们以智谱 Embedding API 为例，介绍了如何将其他 Embedding API 封装到 LangChain\n",
    "中，欢迎感兴趣的读者阅读。\n",
    "\n",
    "**注：如果你使用智谱 API，你可以参考讲解内容实现封装代码，也可以直接使用我们已经封装好的代码[zhipuai_embedding.py](./zhipuai_embedding.py)，将该代码同样下载到本 Notebook 的同级目录，就可以直接导入我们封装的函数。在下面的代码 Cell 中，我们默认使用了智谱的 Embedding，将其他两种 Embedding 使用代码以注释的方法呈现，如果你使用的是百度 API 或者 OpenAI API，可以根据情况来使用下方 Cell 中的代码。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 OpenAI Embedding\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# 使用百度千帆 Embedding\n",
    "# from langchain.embeddings.baidu_qianfan_endpoint import QianfanEmbeddingsEndpoint\n",
    "# 使用我们自己封装的智谱 Embedding，需要将封装代码下载到本地使用\n",
    "from zhipuai_embedding import ZhipuAIEmbeddings\n",
    "\n",
    "# 定义 Embeddings\n",
    "# embedding = OpenAIEmbeddings() \n",
    "embedding = ZhipuAIEmbeddings()\n",
    "# embedding = QianfanEmbeddingsEndpoint()\n",
    "\n",
    "# 定义持久化路径\n",
    "persist_directory = '../../data_base/vector_db/chroma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf '../../data_base/vector_db/chroma'  # 删除旧的数据库文件（如果文件夹中有文件的话），windows电脑请手动删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此之后，我们要确保通过运行 vectordb.persist 来持久化向量数据库，以便我们在未来的课程中使用。\n",
    "\n",
    "让我们保存它，以便以后使用！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量库中存储的数量：925\n"
     ]
    }
   ],
   "source": [
    "print(f\"向量库中存储的数量：{vectordb._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、向量检索\n",
    "### 3.1 相似度检索\n",
    "Chroma的相似度搜索使用的是余弦距离，即：\n",
    "$$\n",
    "similarity = cos(A, B) = \\frac{A \\cdot B}{\\parallel A \\parallel \\parallel B \\parallel} = \\frac{\\sum_1^n a_i b_i}{\\sqrt{\\sum_1^n a_i^2}\\sqrt{\\sum_1^n b_i^2}}\n",
    "$$\n",
    "其中$a_i$、$b_i$分别是向量$A$、$B$的分量。\n",
    "\n",
    "当你需要数据库返回严谨的按余弦相似度排序的结果时可以使用`similarity_search`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"什么是大语言模型\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的内容数：3\n"
     ]
    }
   ],
   "source": [
    "sim_docs = vectordb.similarity_search(question,k=3)\n",
    "print(f\"检索到的内容数：{len(sim_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的第0个内容: \n",
      "第六章 文本转换\n",
      "\n",
      "大语言模型具有强大的文本转换能力，可以实现多语言翻译、拼写纠正、语法调整、格式转换等不同类型的文本转换任务。利用语言模型进行各类转换是它的典型应用之一。\n",
      "\n",
      "在本章中,我们将介绍如何通过编程调用API接口，使用语言模型实现文本转换功能。通过代码示例，读者可以学习将输入文本转换成所需输出格式的具体方法。\n",
      "\n",
      "掌握调用大语言模型接口进行文本转换的技能，是开发各种语言类应用的重要一步。文\n",
      "--------------\n",
      "检索到的第1个内容: \n",
      "以英译汉为例，传统统计机器翻译多倾向直接替换英文词汇，语序保持英语结构，容易出现中文词汇使用不地道、语序不顺畅的现象。而大语言模型可以学习英汉两种语言的语法区别，进行动态的结构转换。同时，它还可以通过上下文理解原句意图，选择合适的中文词汇进行转换，而非生硬的字面翻译。\n",
      "\n",
      "大语言模型翻译的这些优势使其生成的中文文本更加地道、流畅，兼具准确的意义表达。利用大语言模型翻译，我们能够打通多语言之间的壁垒，\n",
      "--------------\n",
      "检索到的第2个内容: \n",
      "通过这个例子，我们可以看到大语言模型可以流畅地处理多个转换要求，实现中文翻译、拼写纠正、语气升级和格式转换等功能。\n",
      "\n",
      "利用大语言模型强大的组合转换能力，我们可以避免多次调用模型来进行不同转换，极大地简化了工作流程。这种一次性实现多种转换的方法，可以广泛应用于文本处理与转换的场景中。\n",
      "\n",
      "六、英文版\n",
      "\n",
      "1.1 翻译为西班牙语\n",
      "\n",
      "python\n",
      "prompt = f\"\"\"\n",
      "Translate the fo\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for i, sim_doc in enumerate(sim_docs):\n",
    "    print(f\"检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 MMR检索\n",
    "如果只考虑检索出内容的相关性会导致内容过于单一，可能丢失重要信息。\n",
    "\n",
    "最大边际相关性 (`MMR, Maximum marginal relevance`) 可以帮助我们在保持相关性的同时，增加内容的丰富度。\n",
    "\n",
    "核心思想是在已经选择了一个相关性高的文档之后，再选择一个与已选文档相关性较低但是信息丰富的文档。这样可以在保持相关性的同时，增加内容的多样性，避免过于单一的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr_docs = vectordb.max_marginal_relevance_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMR 检索到的第0个内容: \n",
      "第六章 文本转换\n",
      "\n",
      "大语言模型具有强大的文本转换能力，可以实现多语言翻译、拼写纠正、语法调整、格式转换等不同类型的文本转换任务。利用语言模型进行各类转换是它的典型应用之一。\n",
      "\n",
      "在本章中,我们将介绍如何通过编程调用API接口，使用语言模型实现文本转换功能。通过代码示例，读者可以学习将输入文本转换成所需输出格式的具体方法。\n",
      "\n",
      "掌握调用大语言模型接口进行文本转换的技能，是开发各种语言类应用的重要一步。文\n",
      "--------------\n",
      "MMR 检索到的第1个内容: \n",
      "与基础语言模型不同，指令微调 LLM 通过专门的训练，可以更好地理解并遵循指令。举个例子，当询问“法国的首都是什么？”时，这类模型很可能直接回答“法国的首都是巴黎”。指令微调 LLM 的训练通常基于预训练语言模型，先在大规模文本数据上进行预训练，掌握语言的基本规律。在此基础上进行进一步的训练与微调（finetune），输入是指令，输出是对这些指令的正确回复。有时还会采用RLHF（reinforce\n",
      "--------------\n",
      "MMR 检索到的第2个内容: \n",
      "有能够拟合训练集的模型构成的集合称为“版本空间”\n",
      "。\n",
      "1.4\n",
      "归纳偏好\n",
      "在上一节“房价预测”的例子中，当选用一元线性回归算法时，学得的模型是一元一次函数，当选\n",
      "用多项式回归算法时，学得的模型是一元二次函数，所以不同的机器学习算法有不同的偏好，我们称为\n",
      "“归纳偏好”\n",
      "。对于当前房价预测这个例子来说，这两个算法学得的模型哪个更好呢？著名的“奥卡姆剃\n",
      "刀”原则认为“若有多个假设与观察一致，则选最简单的那\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for i, sim_doc in enumerate(mmr_docs):\n",
    "    print(f\"MMR 检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_universe_2.x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
